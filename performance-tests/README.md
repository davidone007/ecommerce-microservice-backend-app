# Pruebas de Rendimiento y EstrÃ©s - E-commerce Microservices

## ğŸ“‹ Ãndice

- [DescripciÃ³n](#descripciÃ³n)
- [CaracterÃ­sticas](#caracterÃ­sticas)
- [Requisitos Previos](#requisitos-previos)
- [InstalaciÃ³n](#instalaciÃ³n)
- [Uso](#uso)
- [Tipos de Pruebas](#tipos-de-pruebas)
- [MÃ©tricas Clave](#mÃ©tricas-clave)
- [ConfiguraciÃ³n](#configuraciÃ³n)
- [Resultados](#resultados)
- [Docker](#docker)
- [Mejores PrÃ¡cticas](#mejores-prÃ¡cticas)

## ğŸ¯ DescripciÃ³n

Suite completa de pruebas de rendimiento y estrÃ©s para el sistema de e-commerce basado en microservicios. Utiliza **Locust** para simular casos de uso reales y medir el comportamiento del sistema bajo diferentes condiciones de carga.

## âœ¨ CaracterÃ­sticas

- **Escenarios Realistas**: SimulaciÃ³n de usuarios navegando, comprando y gestionando favoritos
- **MÃºltiples Tipos de Pruebas**: Smoke, Load, Stress, Spike y Soak tests
- **MÃ©tricas Detalladas**: Response time, throughput, error rate, percentiles
- **Reportes HTML**: VisualizaciÃ³n interactiva de resultados con grÃ¡ficos
- **AutenticaciÃ³n JWT**: Manejo automÃ¡tico de tokens de autenticaciÃ³n
- **Modo Distribuido**: Soporte para pruebas distribuidas con Docker
- **Interfaz Web**: Dashboard interactivo en tiempo real

## ğŸ“¦ Requisitos Previos

### Software Necesario

- Python 3.11+
- pip (gestor de paquetes de Python)
- Docker & Docker Compose (opcional, para ejecuciÃ³n en contenedores)
- Sistema de microservicios ejecutÃ¡ndose (ver `compose.yml` en la raÃ­z)

### Servicios del Sistema

Antes de ejecutar las pruebas, asegÃºrate de que los microservicios estÃ©n corriendo:

```bash
# Desde la raÃ­z del proyecto
docker-compose -f compose.yml up -d
```

Verifica que los servicios estÃ©n disponibles:

```bash
curl http://localhost:8080/app/api/products
```

## ğŸš€ InstalaciÃ³n

### OpciÃ³n 1: InstalaciÃ³n Local

```bash
# 1. Navegar al directorio de pruebas
cd performance-tests

# 2. Crear entorno virtual (recomendado)
python3 -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate

# 3. Instalar dependencias
pip install -r requirements.txt

# 4. Dar permisos de ejecuciÃ³n a scripts
chmod +x scripts/*.sh
```

### OpciÃ³n 2: Usando Docker

```bash
# 1. Construir imagen
docker build -t ecommerce-locust .

# 2. O usar Docker Compose
docker-compose up -d
```

## ğŸ’» Uso

### MenÃº Interactivo (Recomendado)

```bash
./scripts/run-tests.sh
```

Este comando abrirÃ¡ un menÃº interactivo donde puedes seleccionar el tipo de prueba:

```
1) ğŸ”¥ Smoke Test       - VerificaciÃ³n bÃ¡sica (2 min, 5 usuarios)
2) ğŸ“Š Load Test        - Carga normal (10 min, 50 usuarios)
3) ğŸ’ª Stress Test      - Prueba de estrÃ©s (15 min, 200 usuarios)
4) âš¡ Spike Test       - Picos de trÃ¡fico (3 min, 300 usuarios)
5) ğŸŠ Soak Test        - Resistencia (30 min, 100 usuarios)
6) ğŸŒ Web UI           - Modo interactivo
7) ğŸš€ Todas las pruebas
8) ğŸ” Ver Ãºltimos resultados
9) âŒ Salir
```

### Comandos Directos

#### Prueba de Carga BÃ¡sica

```bash
locust -f locustfile.py \
    --host=http://localhost:8080 \
    --users 50 \
    --spawn-rate 5 \
    --run-time 5m \
    --headless
```

#### Modo Web UI (Interactivo)

```bash
locust -f locustfile.py --host=http://localhost:8080
```

Luego abre tu navegador en: `http://localhost:8089`

#### Generar Reportes

```bash
# Durante la ejecuciÃ³n, especifica archivos de salida
locust -f locustfile.py \
    --host=http://localhost:8080 \
    --users 50 \
    --spawn-rate 5 \
    --run-time 5m \
    --headless \
    --html=results/report.html \
    --csv=results/stats
```

#### Analizar Resultados

```bash
python scripts/analyze_results.py results/stats_stats.csv --output results/analysis.html
```

## ğŸ§ª Tipos de Pruebas

### 1. ğŸ”¥ Smoke Test (Prueba de Humo)

**Objetivo**: VerificaciÃ³n rÃ¡pida de que el sistema funciona

- **DuraciÃ³n**: 2 minutos
- **Usuarios**: 5
- **Uso**: ValidaciÃ³n post-deployment

```bash
locust -f locustfile.py --host=http://localhost:8080 --users 5 --spawn-rate 1 --run-time 2m --headless
```

### 2. ğŸ“Š Load Test (Prueba de Carga)

**Objetivo**: Evaluar comportamiento bajo carga normal esperada

- **DuraciÃ³n**: 10 minutos
- **Usuarios**: 50
- **Uso**: Validar rendimiento en operaciÃ³n normal

```bash
locust -f locustfile.py --host=http://localhost:8080 --users 50 --spawn-rate 5 --run-time 10m --headless
```

### 3. ğŸ’ª Stress Test (Prueba de EstrÃ©s)

**Objetivo**: Identificar lÃ­mites del sistema y puntos de quiebre

- **DuraciÃ³n**: 15 minutos
- **Usuarios**: 200
- **Uso**: Encontrar capacidad mÃ¡xima

```bash
locust -f locustfile.py --host=http://localhost:8080 --users 200 --spawn-rate 10 --run-time 15m --headless
```

### 4. âš¡ Spike Test (Prueba de Picos)

**Objetivo**: Evaluar respuesta ante aumentos sÃºbitos de trÃ¡fico

- **DuraciÃ³n**: 3 minutos
- **Usuarios**: 300
- **Uso**: Simular eventos como Black Friday

```bash
locust -f locustfile.py --host=http://localhost:8080 --users 300 --spawn-rate 50 --run-time 3m --headless
```

### 5. ğŸŠ Soak Test (Prueba de Resistencia)

**Objetivo**: Evaluar estabilidad bajo carga prolongada

- **DuraciÃ³n**: 30 minutos
- **Usuarios**: 100
- **Uso**: Detectar memory leaks y degradaciÃ³n

```bash
locust -f locustfile.py --host=http://localhost:8080 --users 100 --spawn-rate 5 --run-time 30m --headless
```

## ğŸ“Š MÃ©tricas Clave

### Tiempo de Respuesta

- **Average Response Time**: Tiempo promedio de respuesta
- **Median (P50)**: 50% de las requests estÃ¡n por debajo de este tiempo
- **P95**: 95% de las requests estÃ¡n por debajo de este tiempo
- **P99**: 99% de las requests estÃ¡n por debajo de este tiempo

### Throughput

- **RPS (Requests per Second)**: Cantidad de requests procesadas por segundo
- **Total Requests**: Total de requests ejecutadas
- **Requests/s**: Tasa de requests en tiempo real

### Confiabilidad

- **Success Rate**: Porcentaje de requests exitosas
- **Failure Rate**: Porcentaje de requests fallidas
- **Error Distribution**: DistribuciÃ³n de tipos de error

### Thresholds Recomendados

```yaml
Excelente:
  - Response Time: < 200ms
  - Success Rate: > 99.5%
  - Throughput: > 50 RPS

Bueno:
  - Response Time: < 500ms
  - Success Rate: > 99%
  - Throughput: > 30 RPS

Aceptable:
  - Response Time: < 1000ms
  - Success Rate: > 95%
  - Throughput: > 10 RPS
```

## âš™ï¸ ConfiguraciÃ³n

### Archivo de ConfiguraciÃ³n

Edita `config/test-config.yaml` para ajustar:

```yaml
scenarios:
  load_test:
    users: 50          # NÃºmero de usuarios concurrentes
    spawn_rate: 5      # Usuarios que se agregan por segundo
    duration: "10m"    # DuraciÃ³n de la prueba

performance_thresholds:
  response_time:
    excellent: 200     # ms
    acceptable: 1000   # ms
```

### Personalizar Locustfile

El archivo `locustfile.py` contiene dos clases de usuarios:

- **BrowsingUser**: Usuarios que navegan (peso: 3)
- **BuyingUser**: Usuarios que compran (peso: 1)

Ajusta los pesos para cambiar la distribuciÃ³n:

```python
class BrowsingUser(FastHttpUser):
    weight = 3  # 75% de usuarios

class BuyingUser(FastHttpUser):
    weight = 1  # 25% de usuarios
```

## ğŸ“ Resultados

Los resultados se guardan en el directorio `results/`:

```
results/
â”œâ”€â”€ load_test_20241104_153000.html      # Reporte HTML de Locust
â”œâ”€â”€ load_test_20241104_153000_stats.csv # EstadÃ­sticas CSV
â”œâ”€â”€ load_test_20241104_153000_failures.csv
â”œâ”€â”€ analysis_20241104_153000.html       # AnÃ¡lisis detallado
â””â”€â”€ performance_charts.png              # GrÃ¡ficos
```

### Interpretar Resultados

#### Reporte HTML de Locust

Abre el archivo `.html` en tu navegador para ver:

- Dashboard con mÃ©tricas en tiempo real
- GrÃ¡ficos de requests/segundo
- Tabla de estadÃ­sticas por endpoint
- DistribuciÃ³n de tiempos de respuesta
- Log de errores

#### Reporte de AnÃ¡lisis

El script `analyze_results.py` genera un reporte mejorado con:

- MÃ©tricas consolidadas
- GrÃ¡ficos comparativos
- Estado general del sistema
- Recomendaciones

## ğŸ³ Docker

### EjecuciÃ³n Individual

```bash
# Ejecutar contenedor standalone
docker run -it --rm \
    --network host \
    -v $(pwd)/results:/performance-tests/results \
    ecommerce-locust \
    -f locustfile.py \
    --host=http://localhost:8080 \
    --users 50 \
    --spawn-rate 5 \
    --run-time 5m \
    --headless
```

### Modo Distribuido

Para pruebas de alto volumen, usa Docker Compose con workers:

```bash
# Iniciar cluster (1 master + 2 workers)
docker-compose up -d

# Ver logs
docker-compose logs -f

# Acceder a Web UI
open http://localhost:8089

# Detener
docker-compose down
```

El modo distribuido permite:

- Escalar horizontalmente aÃ±adiendo mÃ¡s workers
- DistribuciÃ³n de carga entre mÃºltiples mÃ¡quinas
- Mayor capacidad de generaciÃ³n de usuarios virtuales

### Escalar Workers

```bash
# AÃ±adir mÃ¡s workers
docker-compose up -d --scale locust-worker=5
```

## ğŸ“š Mejores PrÃ¡cticas

### Antes de las Pruebas

1. **Asegurar Estado Limpio**:
   ```bash
   docker-compose -f ../compose.yml restart
   ```

2. **Verificar Recursos**:
   - CPU: MÃ­nimo 4 cores disponibles
   - RAM: MÃ­nimo 8GB
   - Disco: Espacio para logs y resultados

3. **Configurar Baseline**:
   - Ejecutar smoke test primero
   - Establecer mÃ©tricas de referencia

### Durante las Pruebas

1. **Monitorear Sistema**:
   - MÃ©tricas de Docker: `docker stats`
   - Logs de servicios: `docker-compose logs -f`
   - Actuator endpoints: `http://localhost:8080/actuator/metrics`

2. **No Interferir**:
   - No ejecutar otras aplicaciones pesadas
   - No modificar el sistema durante las pruebas

### DespuÃ©s de las Pruebas

1. **Analizar Resultados**:
   ```bash
   python scripts/analyze_results.py results/latest_stats.csv
   ```

2. **Comparar con Baseline**:
   - Response time: Â¿aumentÃ³ significativamente?
   - Error rate: Â¿estÃ¡ dentro del threshold?
   - Throughput: Â¿cumple con los objetivos?

3. **Documentar Hallazgos**:
   - Anotar configuraciÃ³n utilizada
   - Identificar cuellos de botella
   - Proponer mejoras

### Troubleshooting

#### Error: Connection Refused

```bash
# Verificar que los servicios estÃ©n corriendo
docker-compose -f ../compose.yml ps

# Verificar conectividad
curl http://localhost:8080/app/api/products
```

#### Error: Authentication Failed

El sistema crea usuarios automÃ¡ticamente, pero si hay problemas:

```bash
# Verificar logs del proxy-client
docker-compose -f ../compose.yml logs proxy-client
```

#### Performance Degradation

Si las pruebas son mÃ¡s lentas de lo esperado:

```bash
# Verificar recursos de Docker
docker stats

# Aumentar recursos en Docker Desktop:
# Settings > Resources > Advanced
```

## ğŸ”§ Comandos Ãštiles

```bash
# Ver estadÃ­sticas en tiempo real (durante Web UI)
watch -n 1 'curl -s http://localhost:8089/stats/requests | jq'

# Limpiar resultados antiguos
rm -rf results/*.html results/*.csv

# Ejecutar prueba rÃ¡pida custom
locust -f locustfile.py --host=http://localhost:8080 \
    --users 10 --spawn-rate 2 --run-time 1m --headless

# Ver ayuda de Locust
locust --help
```

## ğŸ“– Referencias

- [DocumentaciÃ³n de Locust](https://docs.locust.io/)
- [Mejores prÃ¡cticas de Performance Testing](https://martinfowler.com/articles/performance-testing.html)
- [Spring Boot Actuator Metrics](https://docs.spring.io/spring-boot/docs/current/reference/html/actuator.html)

## ğŸ¤ Contribuir

Para agregar nuevos escenarios o mejorar las pruebas:

1. Edita `locustfile.py` agregando nuevas tareas
2. Actualiza `test-config.yaml` con nueva configuraciÃ³n
3. Documenta los cambios en este README
4. Prueba los cambios con smoke test

## ğŸ“ Licencia

Este proyecto es parte del sistema E-commerce Microservices.

---

**Autor**: Performance Testing Suite  
**Fecha**: 2024-11-04  
**VersiÃ³n**: 1.0.0
